{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11848,"databundleVersionId":862157,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:03:04.546663Z","iopub.execute_input":"2025-11-10T15:03:04.547161Z","iopub.status.idle":"2025-11-10T15:03:04.552055Z","shell.execute_reply.started":"2025-11-10T15:03:04.547136Z","shell.execute_reply":"2025-11-10T15:03:04.551126Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport copy\nimport os\nimport torch\nfrom PIL import Image\nfrom PIL import Image, ImageDraw\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn as nn\nfrom torchvision import utils\n%matplotlib inline\nimport sklearn\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom PIL import Image\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.optim as optim\nimport cv2\nfrom torch.utils.data import DataLoader, SubsetRandomSampler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:27:13.171690Z","iopub.execute_input":"2025-11-13T16:27:13.172083Z","iopub.status.idle":"2025-11-13T16:27:23.676417Z","shell.execute_reply.started":"2025-11-13T16:27:13.172054Z","shell.execute_reply":"2025-11-13T16:27:23.675415Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"Load Dataset","metadata":{}},{"cell_type":"code","source":"print(len(os.listdir('../input/histopathologic-cancer-detection/train')))\nprint(len(os.listdir('../input/histopathologic-cancer-detection/test')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:27:23.677784Z","iopub.execute_input":"2025-11-13T16:27:23.678244Z","iopub.status.idle":"2025-11-13T16:27:33.620139Z","shell.execute_reply.started":"2025-11-13T16:27:23.678215Z","shell.execute_reply":"2025-11-13T16:27:33.619160Z"}},"outputs":[{"name":"stdout","text":"220025\n57458\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"train_dir = '../input/histopathologic-cancer-detection/train'\nfile = os.listdir(train_dir)[0]\n\nimg = Image.open(os.path.join(train_dir, file))\narr = np.array(img)\nprint(arr.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:27:33.621188Z","iopub.execute_input":"2025-11-13T16:27:33.621523Z","iopub.status.idle":"2025-11-13T16:27:36.583422Z","shell.execute_reply.started":"2025-11-13T16:27:33.621495Z","shell.execute_reply":"2025-11-13T16:27:36.582190Z"}},"outputs":[{"name":"stdout","text":"(96, 96, 3)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"test_dir = '../input/histopathologic-cancer-detection/test'\nfile = os.listdir(test_dir)[0]\n\nimg = Image.open(os.path.join(test_dir, file))\narr = np.array(img)\nprint(arr.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:27:36.585744Z","iopub.execute_input":"2025-11-13T16:27:36.586146Z","iopub.status.idle":"2025-11-13T16:27:36.617830Z","shell.execute_reply.started":"2025-11-13T16:27:36.586120Z","shell.execute_reply":"2025-11-13T16:27:36.616873Z"}},"outputs":[{"name":"stdout","text":"(96, 96, 3)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_labels = pd.read_csv('/kaggle/input/histopathologic-cancer-detection/train_labels.csv')\nprint(train_labels.head().to_markdown())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:27:36.618633Z","iopub.execute_input":"2025-11-13T16:27:36.618925Z","iopub.status.idle":"2025-11-13T16:27:37.140770Z","shell.execute_reply.started":"2025-11-13T16:27:36.618903Z","shell.execute_reply":"2025-11-13T16:27:37.139870Z"}},"outputs":[{"name":"stdout","text":"|    | id                                       |   label |\n|---:|:-----------------------------------------|--------:|\n|  0 | f38a6374c348f90b587e046aac6079959adf3835 |       0 |\n|  1 | c18f2d887b7ae4f6742ee445113fa1aef383ed77 |       1 |\n|  2 | 755db6279dae599ebb4d39a9123cce439965282d |       0 |\n|  3 | bc3f0c64fb968ff4a8bd33af6971ecae77c75e08 |       0 |\n|  4 | 068aba587a4950175d04c680d38943fd488d6a9d |       0 |\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(train_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:27:37.141730Z","iopub.execute_input":"2025-11-13T16:27:37.142156Z","iopub.status.idle":"2025-11-13T16:27:37.160821Z","shell.execute_reply.started":"2025-11-13T16:27:37.142127Z","shell.execute_reply":"2025-11-13T16:27:37.159959Z"}},"outputs":[{"name":"stdout","text":"                                              id  label\n0       f38a6374c348f90b587e046aac6079959adf3835      0\n1       c18f2d887b7ae4f6742ee445113fa1aef383ed77      1\n2       755db6279dae599ebb4d39a9123cce439965282d      0\n3       bc3f0c64fb968ff4a8bd33af6971ecae77c75e08      0\n4       068aba587a4950175d04c680d38943fd488d6a9d      0\n...                                          ...    ...\n220020  53e9aa9d46e720bf3c6a7528d1fca3ba6e2e49f6      0\n220021  d4b854fe38b07fe2831ad73892b3cec877689576      1\n220022  3d046cead1a2a5cbe00b2b4847cfb7ba7cf5fe75      0\n220023  f129691c13433f66e1e0671ff1fe80944816f5a2      0\n220024  a81f84895ddcd522302ddf34be02eb1b3e5af1cb      1\n\n[220025 rows x 2 columns]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Clean up- Check duplicate ids","metadata":{}},{"cell_type":"code","source":"# Check duplicate rows \ndup_rows = train_labels[train_labels.duplicated(keep=False)]\nprint(\"Duplicate rows:\", len(dup_rows))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:27:37.161691Z","iopub.execute_input":"2025-11-13T16:27:37.161937Z","iopub.status.idle":"2025-11-13T16:27:37.253053Z","shell.execute_reply.started":"2025-11-13T16:27:37.161916Z","shell.execute_reply":"2025-11-13T16:27:37.252163Z"}},"outputs":[{"name":"stdout","text":"Duplicate rows: 0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"When the Label is 1 -> positive and when the label is 0 -> negative","metadata":{}},{"cell_type":"code","source":"imgpath =\"/kaggle/input/histopathologic-cancer-detection/train/\" # training data is stored in this folder\ndf_positive = train_labels.loc[train_labels['label']==1]['id'].values   \ndf_negative = train_labels.loc[train_labels['label']==0]['id'].values       \n\nprint('Positive ids')\nprint(df_positive[0:3],'\\n')\n\nprint('Negative ids')\nprint(df_negative[0:3])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:27:37.254013Z","iopub.execute_input":"2025-11-13T16:27:37.254353Z","iopub.status.idle":"2025-11-13T16:27:37.273999Z","shell.execute_reply.started":"2025-11-13T16:27:37.254322Z","shell.execute_reply":"2025-11-13T16:27:37.273097Z"}},"outputs":[{"name":"stdout","text":"Positive ids\n['c18f2d887b7ae4f6742ee445113fa1aef383ed77'\n 'a24ce148f6ffa7ef8eefb4efb12ebffe8dd700da'\n '7f6ccae485af121e0b6ee733022e226ee6b0c65f'] \n\nNegative ids\n['f38a6374c348f90b587e046aac6079959adf3835'\n '755db6279dae599ebb4d39a9123cce439965282d'\n 'bc3f0c64fb968ff4a8bd33af6971ecae77c75e08']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Visulize Image","metadata":{}},{"cell_type":"code","source":"def plot_fig(ids,title,nrows=5,ncols=15):\n\n    fig,ax = plt.subplots(nrows,ncols,figsize=(18,6))\n    plt.subplots_adjust(wspace=0, hspace=0) \n    for i,j in enumerate(ids[:nrows*ncols]):\n        fname = os.path.join(imgpath ,j +'.tif')\n        img = Image.open(fname)\n        idcol = ImageDraw.Draw(img)\n        idcol.rectangle(((0,0),(95,95)),outline='white')\n        plt.subplot(nrows, ncols, i+1) \n        plt.imshow(np.array(img))\n        plt.axis('off')\n\n    plt.suptitle(title, y=0.94)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:27:37.274874Z","iopub.execute_input":"2025-11-13T16:27:37.275128Z","iopub.status.idle":"2025-11-13T16:27:37.281342Z","shell.execute_reply.started":"2025-11-13T16:27:37.275108Z","shell.execute_reply":"2025-11-13T16:27:37.280354Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Due to the size of the file, I didn't show the image of the cases","metadata":{}},{"cell_type":"code","source":"# plot_fig(df_positive,'Positive Cases')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:27:37.283660Z","iopub.execute_input":"2025-11-13T16:27:37.284359Z","iopub.status.idle":"2025-11-13T16:27:37.299579Z","shell.execute_reply.started":"2025-11-13T16:27:37.284326Z","shell.execute_reply":"2025-11-13T16:27:37.298564Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# plot_fig(df_negative,'Negative Cases')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:27:37.300634Z","iopub.execute_input":"2025-11-13T16:27:37.301097Z","iopub.status.idle":"2025-11-13T16:27:37.315729Z","shell.execute_reply.started":"2025-11-13T16:27:37.301067Z","shell.execute_reply":"2025-11-13T16:27:37.314697Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Sampling ","metadata":{}},{"cell_type":"code","source":"# Number of samples in each class\nsample_size = 4000\n\n# Data paths\ntrain_dir = '../input/histopathologic-cancer-detection/train'\ntest_dir = '../input/histopathologic-cancer-detection/test'\n\n# Use 80000 positive and negative examples\ndf_negatives = train_labels[train_labels['label'] == 0].sample(sample_size, random_state=42)\ndf_positives = train_labels[train_labels['label'] == 1].sample(sample_size, random_state=42)\n\n# Concatenate the two dfs and shuffle them up\ntrain_df = sklearn.utils.shuffle(pd.concat([df_positives, df_negatives], axis=0).reset_index(drop=True))\n\ntrain_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:33:51.722450Z","iopub.execute_input":"2025-11-10T15:33:51.722741Z","iopub.status.idle":"2025-11-10T15:33:52.154205Z","shell.execute_reply.started":"2025-11-10T15:33:51.722721Z","shell.execute_reply":"2025-11-10T15:33:52.153166Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(8000, 2)"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"Data Pre-processing ","metadata":{}},{"cell_type":"code","source":"class CreateDataset(Dataset):\n    def __init__(self, df_data, data_dir = './', transform=None):\n        super().__init__()\n        self.df = df_data.values\n        self.data_dir = data_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_name,label = self.df[index]\n        img_path = os.path.join(self.data_dir, img_name+'.tif')\n        image = cv2.imread(img_path)\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:38:24.558710Z","iopub.execute_input":"2025-11-10T15:38:24.559120Z","iopub.status.idle":"2025-11-10T15:38:24.565671Z","shell.execute_reply.started":"2025-11-10T15:38:24.559094Z","shell.execute_reply":"2025-11-10T15:38:24.564437Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"transforms_train = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(p=0.4),\n    transforms.RandomVerticalFlip(p=0.4),\n    transforms.RandomRotation(20),\n    transforms.ToTensor(),\n    # We the get the following mean and std for the channels of all the images\n    #transforms.Normalize((0.70244707, 0.54624322, 0.69645334), (0.23889325, 0.28209431, 0.21625058))\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_data = CreateDataset(df_data=train_df, data_dir=train_dir, transform=transforms_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:38:36.712930Z","iopub.execute_input":"2025-11-10T15:38:36.713241Z","iopub.status.idle":"2025-11-10T15:38:36.720477Z","shell.execute_reply.started":"2025-11-10T15:38:36.713218Z","shell.execute_reply":"2025-11-10T15:38:36.719326Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Set Batch Size\nbatch_size = 128\n\n# Percentage of training set to use as validation\nvalid_size = 0.1\n\n# obtain training indices that will be used for validation\nnum_train = len(train_data)\nindices = list(range(num_train))\n# np.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# Create Samplers\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\nvalid_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:42:56.778234Z","iopub.execute_input":"2025-11-10T15:42:56.779232Z","iopub.status.idle":"2025-11-10T15:42:56.785376Z","shell.execute_reply.started":"2025-11-10T15:42:56.779203Z","shell.execute_reply":"2025-11-10T15:42:56.784395Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"Model Architecture","metadata":{}},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN,self).__init__()\n        # Convolutional and Pooling Layers\n        self.conv1=nn.Sequential(\n                nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,stride=1,padding=0),\n                nn.BatchNorm2d(32),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(2,2))\n        self.conv2=nn.Sequential(\n                nn.Conv2d(in_channels=32,out_channels=64,kernel_size=2,stride=1,padding=1),\n                nn.BatchNorm2d(64),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(2,2))\n        self.conv3=nn.Sequential(\n                nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1),\n                nn.BatchNorm2d(128),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(2,2))\n        self.conv4=nn.Sequential(\n                nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=1,padding=1),\n                nn.BatchNorm2d(256),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(2,2))\n        self.conv5=nn.Sequential(\n                nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(2,2))\n        \n        self.dropout2d = nn.Dropout2d()\n        \n        \n        self.fc=nn.Sequential(\n                nn.Linear(512*3*3,1024),\n                nn.ReLU(inplace=True),\n                nn.Dropout(0.4),\n                nn.Linear(1024,512),\n                nn.Dropout(0.4),\n                nn.Linear(512, 1),\n                nn.Sigmoid())\n        \n    def forward(self,x):\n        \"\"\"Method for Forward Prop\"\"\"\n        x=self.conv1(x)\n        x=self.conv2(x)\n        x=self.conv3(x)\n        x=self.conv4(x)\n        x=self.conv5(x)\n        #print(x.shape) <-- Life saving debugging step :D\n        x=x.view(x.shape[0],-1)\n        x=self.fc(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:43:00.263785Z","iopub.execute_input":"2025-11-10T15:43:00.264414Z","iopub.status.idle":"2025-11-10T15:43:00.274092Z","shell.execute_reply.started":"2025-11-10T15:43:00.264390Z","shell.execute_reply":"2025-11-10T15:43:00.273012Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"Check if GPU is available, otherwise train on CPU","metadata":{}},{"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:43:05.231510Z","iopub.execute_input":"2025-11-10T15:43:05.231806Z","iopub.status.idle":"2025-11-10T15:43:05.240127Z","shell.execute_reply.started":"2025-11-10T15:43:05.231783Z","shell.execute_reply":"2025-11-10T15:43:05.239303Z"}},"outputs":[{"name":"stdout","text":"CUDA is not available.  Training on CPU ...\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"Training ","metadata":{}},{"cell_type":"code","source":"model = CNN()\nprint(model)\n\n# Move model to GPU if available\nif train_on_gpu: model.cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:43:17.177894Z","iopub.execute_input":"2025-11-10T15:43:17.178178Z","iopub.status.idle":"2025-11-10T15:43:17.255365Z","shell.execute_reply.started":"2025-11-10T15:43:17.178159Z","shell.execute_reply":"2025-11-10T15:43:17.254436Z"}},"outputs":[{"name":"stdout","text":"CNN(\n  (conv1): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv2): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv3): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv4): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv5): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (dropout2d): Dropout2d(p=0.5, inplace=False)\n  (fc): Sequential(\n    (0): Linear(in_features=4608, out_features=1024, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.4, inplace=False)\n    (3): Linear(in_features=1024, out_features=512, bias=True)\n    (4): Dropout(p=0.4, inplace=False)\n    (5): Linear(in_features=512, out_features=1, bias=True)\n    (6): Sigmoid()\n  )\n)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Trainable Parameters\npytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(\"Number of trainable parameters: \\n{}\".format(pytorch_total_params))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:43:22.419855Z","iopub.execute_input":"2025-11-10T15:43:22.420162Z","iopub.status.idle":"2025-11-10T15:43:22.425027Z","shell.execute_reply.started":"2025-11-10T15:43:22.420140Z","shell.execute_reply":"2025-11-10T15:43:22.424154Z"}},"outputs":[{"name":"stdout","text":"Number of trainable parameters: \n6805249\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# specify loss function (categorical cross-entropy loss)\ncriterion = nn.BCELoss()\n\n# specify optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.00015)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:43:38.562098Z","iopub.execute_input":"2025-11-10T15:43:38.562870Z","iopub.status.idle":"2025-11-10T15:43:38.567221Z","shell.execute_reply.started":"2025-11-10T15:43:38.562842Z","shell.execute_reply":"2025-11-10T15:43:38.566441Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"Choose a small n_epochs due to no GPU","metadata":{}},{"cell_type":"code","source":"# number of epochs to train the model\nn_epochs = 5\n\nvalid_loss_min = np.Inf\n\n# keeping track of losses as it happen\ntrain_losses = []\nvalid_losses = []\nval_auc = []\ntest_accuracies = []\nvalid_accuracies = []\nauc_epoch = []\n\nfor epoch in range(1, n_epochs+1):\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n  \n    # train the model #\n    \n    model.train()\n    for data, target in train_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda().float()\n        target = target.view(-1, 1).float()\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # Update Train loss and accuracies\n        train_loss += loss.item()*data.size(0)\n        print(loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:43:40.634936Z","iopub.execute_input":"2025-11-10T15:43:40.635225Z"}},"outputs":[{"name":"stdout","text":"tensor(0.7140, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.6991, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.6492, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.6135, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.5585, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.5648, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.5269, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.4671, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.4640, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.4846, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.4720, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.4054, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.5532, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.5153, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.5683, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.4470, grad_fn=<BinaryCrossEntropyBackward0>)\ntensor(0.4264, grad_fn=<BinaryCrossEntropyBackward0>)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Validation","metadata":{}},{"cell_type":"code","source":"model.eval()\n    for data, target in valid_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda().float()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        target = target.view(-1, 1).float()\n        output = model(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # update average validation loss \n        valid_loss += loss.item()*data.size(0)\n        #output = output.topk()\n        y_actual = target.data.cpu().numpy()\n        y_pred = output[:,-1].detach().cpu().numpy()\n        val_auc.append(roc_auc_score(y_actual, y_pred))        \n    \n    # calculate average losses\n    train_loss = train_loss/len(train_loader.sampler)\n    valid_loss = valid_loss/len(valid_loader.sampler)\n    valid_auc = np.mean(val_auc)\n    auc_epoch.append(np.mean(val_auc))\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n\n# print training/validation statistics \n    print('Epoch: {} | Training Loss: {:.6f} | Validation Loss: {:.6f} | Validation AUC: {:.4f}'.format(\n        epoch, train_loss, valid_loss, valid_auc))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:03:15.693650Z","iopub.status.idle":"2025-11-10T15:03:15.693953Z","shell.execute_reply.started":"2025-11-10T15:03:15.693783Z","shell.execute_reply":"2025-11-10T15:03:15.693797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}